from google import genai
from google.genai import types
from pathlib import Path
import pdfplumber
import os
from dotenv import load_dotenv
from loguru import logger as log
from qdrant_client import QdrantClient, models
from qdrant_client.models import Distance, VectorParams
from tqdm import tqdm
from langchain_text_splitters import MarkdownHeaderTextSplitter


load_dotenv()

EMBEDDING_MODEL_ID = "text-embedding-004"  # text-embedding-004 / gemini-embedding-001

QDRANT_HOST = "qdrant"
QDRANT_PORT = 6333
COLLECTION_NAME = "sleep_ai_knowledge_base"
VECTOR_DIMENSION = 768  # 768 –¥–ª—è text-embedding-004, 3072 –¥–ª—è gemini-embedding-001
BATCH_SIZE = 150

client = genai.Client(api_key=os.getenv("GEMINI_API_KEY"))
qdrant_client = QdrantClient(host=QDRANT_HOST, port=QDRANT_PORT)
text_splitter = MarkdownHeaderTextSplitter(
    headers_to_split_on=[
        ('##', 'advice'), 
        ('###', 'longrid')
    ]
)


class SleepAiRagEmbeddingConfig:
    @staticmethod
    def run_pipeline(file_paths: list[Path]):
        if not qdrant_client.collection_exists(collection_name=COLLECTION_NAME):
            log.info(f"–°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏: {COLLECTION_NAME}")
            qdrant_client.recreate_collection(
                collection_name=COLLECTION_NAME,
                vectors_config=VectorParams(size=VECTOR_DIMENSION, distance=Distance.COSINE)
            )
        else:
            log.info(f"–ö–æ–ª–ª–µ–∫—Ü–∏—è {COLLECTION_NAME} —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç.")

        # –æ–±—â–∏–π —Å—á—ë—Ç—á–∏–∫ –¥–ª—è –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤
        global_id = 0

        for file in file_paths:
            log.info(f"\nüìò –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞: {file.name}")
            with open(file, "r", encoding="utf-8") as f:
                content = f.read()
            chunks_data, merged_list = SleepAiRagEmbeddingConfig.create_chunks_data(content=content)

            embeddings = SleepAiRagEmbeddingConfig.get_batch_embeddings(merged_list=merged_list)

            all_points = []
            for num, data in enumerate(chunks_data):
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —Ç–æ—á–∫–∞ —É–∂–µ –≤ Qdrant
                exists = qdrant_client.count(
                    collection_name=COLLECTION_NAME,
                    count_filter=models.Filter(
                        must=[
                            models.FieldCondition(
                                key="advice",
                                match=models.MatchValue(value=data["advice"])
                            ),
                            models.FieldCondition(
                                key="chunk_id",
                                match=models.MatchValue(value=data["chunk_id"])
                            ),
                            models.FieldCondition(
                                key="source_file",
                                match=models.MatchValue(value=file.name)
                            )
                        ]
                    )
                )

                if exists.count > 0:
                    log.info(f"‚è© –ü—Ä–æ–ø—É—Å–∫: advice='{data['advice']}', chunk_id={data['chunk_id']} ‚Äî —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")
                    continue

                all_points.append(
                    models.PointStruct(
                        id=global_id + num,
                        vector=embeddings[num],
                        payload={
                            "title_doc": data["title_doc"], # –∑–∞–≥–æ–ª–æ–≤–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–∞
                            "advice": data["advice"],     
                            "text": data["text"],
                            "chunk_id": data["chunk_id"],
                            "source_file": file.name
                        }
                    )
                )
            global_id += len(chunks_data)

            log.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ {len(all_points)} —Ç–æ—á–µ–∫ –≤ Qdrant...")
            for batch_start_index in tqdm(range(0, len(all_points), BATCH_SIZE), desc="Qdrant upload"):
                batch_points = all_points[batch_start_index:batch_start_index + BATCH_SIZE]
                qdrant_client.upsert(
                    collection_name=COLLECTION_NAME,
                    points=batch_points,
                    wait=True
                )

        log.info("\n‚úÖ –ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
        info = qdrant_client.get_collection(collection_name=COLLECTION_NAME)
        log.info(f"–¢–µ–∫—É—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ—á–µ–∫: {info.points_count}")



    @staticmethod
    def create_chunks_data(content: str) -> list:
        chanks = text_splitter.split_text(content)
        merged = {}

        for doc in chanks:
            advice = doc.metadata.get("advice")
            longrid = doc.metadata.get("longrid")
            text = doc.page_content.strip()
            if advice not in merged:
                merged[advice] = {
                    "advice": advice,
                    "advice_text": text if not longrid else "",
                    "longrid": longrid,
                    "longrid_text": text if longrid else "",
                    "chunk_id": len(merged)
                }

            else:
                if longrid:
                    merged[advice]["longrid"] = longrid
                    merged[advice]["longrid_text"] = text
                else:
                    merged[advice]["advice_text"] += f"\n\n{text}"

        merged_list = list(merged.values())
        log.info(f"–û–±—ä–µ–¥–∏–Ω–µ–Ω–æ {len(merged_list)} —Å–æ–≤–µ—Ç–æ–≤.")

        chunks_data = []
        for item in merged_list:
            chunks_data.append({
                "title_doc": "–ö–∞–∫ —É–ª—É—á–∏—à–∏—Ç—å —Å–æ–Ω",
                "advice": item["advice"],
                "text": f"{item['advice']}\n{item['advice_text']}\n\n{item['longrid_text']}".strip(),
                "chunk_id": item["chunk_id"]
            })
        return chunks_data, merged_list


    @staticmethod
    def get_batch_embeddings(merged_list: list) -> list:
        all_embeddings = []
        for i in tqdm(range(0, len(merged_list), BATCH_SIZE), desc="–ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"):
            batch = merged_list[i:i + BATCH_SIZE]
            for item in batch:
                combined_text = f"{item['advice']}\n{item['advice_text']}\n\n{item['longrid']}\n{item['longrid_text']}".strip()
                response = client.models.embed_content(
                    model=EMBEDDING_MODEL_ID,
                    contents=[combined_text],
                    config=types.EmbedContentConfig(
                        task_type="retrieval_document",
                        title=item["advice"]
                    )
                )
                all_embeddings.append(response.embeddings[0].values)
        return all_embeddings




if __name__ == "__main__":
    # qdrant_client.delete_collection(collection_name="sleep_ai_knowledge_base")
    SleepAiRagEmbeddingConfig.run_pipeline(
        file_paths=[
            Path("app/ai/resources/RAG/knowledge_base/–°–æ–Ω.md"),
        ]
    )

